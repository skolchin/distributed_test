name: ray

services:
  server:
    image: ray-server
    hostname: ray-server
    shm_size: '10gb'
    build:
      context: .
      dockerfile: ./Dockerfile.server
    ports:
      - "6379:6379"
    volumes:
      - ray-tmp:/tmp
    user: "${UID:-1000}:${GID:-1000}"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - net
    profiles:
      - server

  vllm:
    image: ray-vllm
    build:
      context: .
      dockerfile: ./Dockerfile.vllm
      args:
        MODEL: ${MODEL:-Qwen/Qwen3-0.6B}
        VLLM_UID: ${UID:-1000}
        VLLM_GID: ${GID:-1000}
    ports:
      - "8000:8000"
    volumes:
      - models:/home/vllm/.cache/huggingface/hub
      - ray-tmp:/tmp
    environment:
      - RAY_ADDRESS=ray-server:6379
      - HUGGINGFACE_API_KEY=${HUGGINGFACE_API_KEY}
      - MODEL=${MODEL:-Qwen/Qwen3-0.6B}
      - NCCL_DEBUG=trace
      - VLLM_LOGGING_LEVEL=debug
      - CUDA_LAUNCH_BLOCKING=1
      - VLLM_TRACE_FUNCTION=0
      - NCCL_P2P_DISABLE=1
      - OMP_NUM_THREADS=2
      - NVIDIA_VISIBLE_DEVICES=all
    user: "${UID:-1000}:${GID:-1000}"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - net
    profiles:
      - vllm
    depends_on:
      - server:

volumes:
  ray-tmp:
  models:

networks:
  net:
    # driver: host
    # attachable: true
