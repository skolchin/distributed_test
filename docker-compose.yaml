name: ray

services:
  server:
    image: ray-server
    hostname: ray-server
    shm_size: '10gb'
    network_mode: host
    build:
      context: .
      dockerfile: ./Dockerfile.server
      args:
        RAY_UID: ${UID:-1000}
        RAY_GID: ${GID:-1000}
    environment:
      - HUGGINGFACE_API_KEY=${HUGGINGFACE_API_KEY}
      - MODEL=${MODEL:-Qwen/Qwen3-0.6B}
    # ports:
    #   - "6379:6379"
    #   - "8000:8000"
    #   - "43403:43403"
    #   - "43404:43404"
    #   - "43405:43405"
    #   - "43406:43406"
    #   - "43407:43407"
    #   - "43408:43408"
    #   - "10000:10000"
    #   - "10001:10001"
    #   - "10002:10002"
    user: "${UID:-1000}:${GID:-1000}"
    volumes:
      - models:/home/vllm/.cache/huggingface/hub
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # networks:
    #   - net
    profiles:
      - server

  client:
    image: ray-client
    hostname: ray-client
    network_mode: host
    shm_size: '10gb'
    build:
      context: .
      dockerfile: ./Dockerfile.client
    environment:
      - RAY_ADDRESS=${HEAD:-192.168.0.7}:6379
    ports:
      - "44403:44403"
      - "44404:44404"
      - "44405:44405"
      - "44406:44406"
      - "44407:44407"
      - "44408:44408"    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    profiles:
      - client

  # vllm:
  #   image: ray-vllm
  #   build:
  #     context: .
  #     dockerfile: ./Dockerfile.vllm
  #     args:
  #       MODEL: ${MODEL:-Qwen/Qwen3-0.6B}
  #       VLLM_UID: ${UID:-1000}
  #       VLLM_GID: ${GID:-1000}
  #   ports:
  #     - "8000:8000"
  #   volumes:
  #     - models:/home/vllm/.cache/huggingface/hub
  #     - ray-tmp:/tmp
  #   environment:
  #     - RAY_ADDRESS=ray-server:6379
  #     - HUGGINGFACE_API_KEY=${HUGGINGFACE_API_KEY}
  #     - MODEL=${MODEL:-Qwen/Qwen3-0.6B}
  #     - NCCL_DEBUG=trace
  #     - VLLM_LOGGING_LEVEL=debug
  #     - CUDA_LAUNCH_BLOCKING=0
  #     - VLLM_TRACE_FUNCTION=0
  #     - NCCL_P2P_DISABLE=1
  #     - OMP_NUM_THREADS=2
  #     - NVIDIA_VISIBLE_DEVICES=all
  #   user: "${UID:-1000}:${GID:-1000}"
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  #   networks:
  #     - net
  #   profiles:
  #     - vllm

volumes:
  models:
  # ray-tmp:

# networks:
#   net:
